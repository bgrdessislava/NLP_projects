{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d51ffd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dessy\\anaconda3\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import urllib3\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "\n",
    "urllib3.disable_warnings() # Fix the warnings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1b2b8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the wb link for all the USA and we change the page numbers\n",
    "base = 'https://scholarshipdb.net/PhD-scholarships-in-United-States?page={}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0b2ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page 800.\n"
     ]
    }
   ],
   "source": [
    "allPhD = defaultdict(list)\n",
    "startPage = 793\n",
    "page = startPage\n",
    "while True:\n",
    "    if page % 10 == 0:\n",
    "        print(f'Processing page {page}.')\n",
    "    url = base.format(page)\n",
    "    html_text = requests.get(url, verify=False)\n",
    "    # Create a parser instance able to parse invalid markup.\n",
    "    soup = BeautifulSoup(html_text.text, 'html.parser')\n",
    "    links = soup.select(\".list-unstyled h4 a\")\n",
    "    if len(links) == 0:\n",
    "        break\n",
    "    for link in links:\n",
    "        link = f'https://scholarshipdb.net{link.get(\"href\")}'\n",
    "        html = requests.get(link, verify=False)\n",
    "        soup2 = BeautifulSoup(html.text, 'html.parser')\n",
    "        allPhD['title'].append(soup2.find('head').title.text)\n",
    "        allPhD['location'].append(soup2.find('h2').find('a').text)\n",
    "        allPhD['keywords'].append(soup2.find('meta',attrs={\"name\": 'keywords'})['content'])\n",
    "        allPhD['meta'].append(soup2.find('meta',attrs={\"name\": 'description'})['content'])\n",
    "        allPhD['description'].append(soup2.find('div', attrs={'class': 'description'}).text.strip())\n",
    "    time.sleep(5)\n",
    "    page += 1\n",
    "allPhD = pd.DataFrame(allPhD)\n",
    "allPhD.to_pickle(f'scraping_scholarshipDB_dataframe-{startPage}-{page}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d3046dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_pickle('scraping_scholarshipDB_dataframe-1-793.pkl')\n",
    "df2 = pd.read_pickle('scraping_scholarshipDB_dataframe-793-1001.pkl')\n",
    "scholarhipDB = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e00b3271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanString(x, regex):\n",
    "    # Make lower case and replace reg with space\n",
    "    x = regex.sub(' ', x.lower())\n",
    "    # Replace multi-spaces with single-space\n",
    "    return ' '.join(x.split())\n",
    "\n",
    "# Replace non-alphabetic characters with single space\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "for col in scholarhipDB.columns:\n",
    "    scholarhipDB[col] = scholarhipDB[col].apply(cleanString, args=(regex,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "31b23e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "scholarhipDB.to_pickle('scraping_scholarshipDB_dataframe.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5389b51a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scholarhipDB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mscholarhipDB\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m20\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scholarhipDB' is not defined"
     ]
    }
   ],
   "source": [
    "scholarhipDB[\"keywords\"].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e0d0ae64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "and                   407128\n",
       "the                   298696\n",
       "of                    250689\n",
       "to                    197762\n",
       "in                    149080\n",
       "                       ...  \n",
       "marty                      1\n",
       "woldorff                   1\n",
       "noblebusiness              1\n",
       "hilllocationchapel         1\n",
       "golang                     1\n",
       "Name: description, Length: 61619, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scholarhipDB[\"description\"].apply(lambda x: x.split()).explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b202acc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "|                    10028\n",
       "United               10005\n",
       "States               10004\n",
       "scholarshipdb.net    10004\n",
       "of                    7058\n",
       "                     ...  \n",
       "I³R                      1\n",
       "(Albuquerque),           1\n",
       "Translation              1\n",
       "Climate,                 1\n",
       "Adversity,               1\n",
       "Name: title, Length: 8148, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scholarhipDB['title'].apply(lambda x: x.split()).explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7c16f06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "student         50140\n",
       "scholarship     50045\n",
       "search          30024\n",
       "scholarships    30013\n",
       "grants          20028\n",
       "                ...  \n",
       "liber               1\n",
       "ero                 1\n",
       "alberta             1\n",
       "eating              1\n",
       "adversity           1\n",
       "Name: keywords, Length: 4198, dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scholarhipDB['keywords'].apply(lambda x: x.split()).explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "476d547c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title          7757\n",
       "location        713\n",
       "keywords       7040\n",
       "meta           7040\n",
       "description    9761\n",
       "dtype: int64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scholarhipDB.drop_duplicates().nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fc9df208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fully funded ms and phd positions are available for spring summer fall in the newly established rock deformation and fault mechanics laboratory in the department of geosciences at utah state university successful candidate s will have the opportunity to work broadly on a experimental studies of rock deformation and earthquake ruptures and or b numerical modeling of earthquake processes depending on the interests of the successful candidate s there may also be opportunities to be involved in field based research with expeditionary scale research initiatives such as the international ocean discovery program iodp students with bachelor s and or master s in the geosciences engineering geotechnical geological petroleum etc or physics with a strong quantitative background are encouraged to apply some prior experience with experimental rock mechanics numerical modeling or machine learning techniques are desirable but not necessary usu and the department of geosciences are committed to inclusivity diversity and antiracism students from groups traditionally underrepresented in the broad field of geosciences are especially encouraged to apply if interested please contact dr srisharan shreedharan srisharan shreedharan usu edu with your cv and a short statement of research interests gre scores are no longer required for admission to the graduate program more information about the application process can be found here https www usu edu geo graduate program future graduates srisharan will be starting in spring as an assistant professor of geomechanics and geophysics in the department of geosciences at utah state university srisharan is broadly interested in faulting friction and the mechanics of how earthquakes start propagate and stop particularly at shallow plate interfaces and in subsurface energy systems srisharan uses a combination of friction experiments ultrasonic monitoring pulse echo and acoustic emissions numerical modeling and machine learning techniques in his research to better understand faulting and friction at multiple scales he is currently a distinguished postdoctoral fellow at the university of texas institute for geophysics srisharan earned his phd in geosciences at penn state and an ms in geological engineering from the university of arizona more about usuusu main campus is located in logan utah a city situated in a picturesque valley which offers numerous year round outdoor activities including local ski resorts climbing biking and hiking trails logan is just a short drive to salt lake city as well as many national parks monuments and conservation areas the logan metropolitan area has been named the second best performing small city in the united states by the milken institute analysis additional information about utah state university https www usu edu and the department of geosciences https www usu edu geo can be found online'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scholarhipDB['description'].values[49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22e7f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import httpx\n",
    "import http3\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import rich\n",
    "from rich.console import Console\n",
    "from datetime import date\n",
    "from dataclasses import dataclass\n",
    "from bs4 import BeautifulSoup as bs\n",
    "#from pathlib import Path\n",
    "#sys.path.append(Path(__file__).parent.parent.as_posix()) # https://stackoverflow.com/questions/16981921\n",
    "from phdseeker.rich_dataframe import prettify\n",
    "from phdseeker.constants import __version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7753d8eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cannot close a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mPhDSeeker.positions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\asyncio\\base_events.py:623\u001b[0m, in \u001b[0;36mBaseEventLoop.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m--> 623\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    625\u001b[0m new_task \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m futures\u001b[38;5;241m.\u001b[39misfuture(future)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\asyncio\\base_events.py:583\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m--> 583\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis event loop is already running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: This event loop is already running",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 229>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    226\u001b[0m     ps\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 230\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    223\u001b[0m keywords \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComputer Science, Machine Learning, Deep Learning\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    225\u001b[0m ps \u001b[38;5;241m=\u001b[39m PhDSeeker(keywords, maxpage\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m--> 226\u001b[0m \u001b[43mps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mPhDSeeker.save\u001b[1;34m(self, output)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, output\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;124;03m\"\"\"Creates excel/csv files based on all revceived data\"\"\"\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositions\u001b[49m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msought_number:\n\u001b[0;32m    200\u001b[0m         s  \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mPhDSeeker.positions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloop\u001b[38;5;241m.\u001b[39mrun_until_complete(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare())\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m positions \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCountry\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcountries,\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdates,\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtitles,\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLink\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinks\n\u001b[0;32m    184\u001b[0m }\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(positions, orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\asyncio\\selector_events.py:84\u001b[0m, in \u001b[0;36mBaseSelectorEventLoop.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m---> 84\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot close a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_closed():\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot close a running event loop"
     ]
    }
   ],
   "source": [
    "# Turns off some warnings\n",
    "import urllib3\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "console = Console()\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    config = {\n",
    "        'scholarshipdb': {\n",
    "            'sought#': 'h1.title',\n",
    "            'query':\n",
    "            'https://scholarshipdb.net/scholarships/Program-PhD?page={page}&q={fields}',\n",
    "            'title': 'h4 a',\n",
    "            'country': '.list-unstyled a.text-success',\n",
    "            'date': '.list-unstyled span.text-muted',\n",
    "            'link': \".list-unstyled h4 a\",\n",
    "        },\n",
    "        'findaphd': {\n",
    "            'sought#': 'h4.course-count.d-none.d-md-block.h6.mb-0.mt-1',\n",
    "            'query':\n",
    "            'https://www.findaphd.com/phds/non-eu-students/?01w0&Keywords={fields}&PG={page}',\n",
    "            'title': \"h4 text-dark mx-0 mb-3\",\n",
    "            'country':\n",
    "            \"country-flag img-responsive phd-result__dept-inst--country-icon\",\n",
    "            'date': \"apply py-2 small\",\n",
    "            'link': \"h4 text-dark mx-0 mb-3\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    def __init__(self, repo='scholarshipdb'):\n",
    "        self.repo = repo\n",
    "\n",
    "    @classmethod\n",
    "    def repos(cls):\n",
    "        return ','.join(list(cls.config))\n",
    "\n",
    "    @property\n",
    "    def query(self):\n",
    "        return Config.config[self.repo]['query']\n",
    "\n",
    "    @property\n",
    "    def sought(self):\n",
    "        return Config.config[self.repo]['sought#']\n",
    "\n",
    "    @property\n",
    "    def title(self):\n",
    "        return Config.config[self.repo]['title']\n",
    "\n",
    "    @property\n",
    "    def link(self):\n",
    "        return Config.config[self.repo]['link']\n",
    "\n",
    "    @property\n",
    "    def country(self):\n",
    "        return Config.config[self.repo]['country']\n",
    "\n",
    "    @property\n",
    "    def date(self):\n",
    "        return Config.config[self.repo]['date']\n",
    "\n",
    "    @property\n",
    "    def baseURL(self):\n",
    "        return next(\n",
    "            re.finditer(r'^.+?[^\\/:](?=[?\\/]|$)',\n",
    "                        Config.config[self.repo]['query'])).group()\n",
    "\n",
    "\n",
    "class PhDSeeker:\n",
    "\n",
    "    def __init__(self,\n",
    "                keywords: str,\n",
    "                repos: str = 'scholarshipdb, findaphd',\n",
    "                maxpage: int = 10):\n",
    "        self.repos = map(str.strip,\n",
    "                        repos.split(','))  # 'scholarshipdb, findaphd'\n",
    "        self.keywords = keywords\n",
    "        self.fields = '%20'.join([\n",
    "            f\"\\\"{item.replace(' ', '%20')}\\\"\"\n",
    "            for item in map(str.strip, keywords.split(','))\n",
    "        ])\n",
    "        self.titles = []\n",
    "        self.countries = []\n",
    "        self.dates = []\n",
    "        self.links = []\n",
    "        self.maxpage = maxpage\n",
    "        self.file_name = f\"PhD_Positions_{date.today()}[{keywords}]\"\n",
    "        self.df = None # DataFrame of found positions\n",
    "        self.sought_number = 0\n",
    "        self.loop = asyncio.get_event_loop()\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.sought_number:\n",
    "            s = 's' if self.maxpage > 1 else ''\n",
    "            caption = \"│\" + f'Out of {self.sought_number} found Ph.D. positions, {len(self.df)} have been fetched in {self.maxpage} page{s}.'.center(console.width-2) + (\n",
    "                '│\\n└' + '─' * (console.width-2) + '┘\\n')\n",
    "            prettify(self.df[['Country', 'Date', 'Title']], clear_console=False)\n",
    "            return f'{caption}'\n",
    "\n",
    "    async def __get_page__(self, repo, page):\n",
    "        headers = {\n",
    "            'user-agent': 'curl/7.83.0',\n",
    "            'accept': '*/*',\n",
    "            'scheme': 'http',\n",
    "            # 'Content-Length': '0',\n",
    "        }\n",
    "        c = Config(repo)\n",
    "        try:\n",
    "            query = c.query.format(fields=self.fields, page=page)\n",
    "            if repo!='findaphd':\n",
    "                client = http3.AsyncClient()\n",
    "                keywords = {'verify': False}\n",
    "            else:\n",
    "                client = httpx.AsyncClient()\n",
    "                keywords = {}\n",
    "            response = await client.get(query,\n",
    "                        headers=headers,\n",
    "                        **keywords,\n",
    "                        )\n",
    "            if isinstance(client, http3.client.AsyncClient):\n",
    "                await client.close()\n",
    "            if isinstance(client, httpx._client.AsyncClient):\n",
    "                await client.aclose()\n",
    "            soup = bs(response.text, \"html.parser\")\n",
    "            if page == 1:  # get the number of sought positions\n",
    "                if (n := soup.select_one(c.sought)) is not None:\n",
    "                    foundPositions = int(re.search('(\\d+[,\\d*]*)', n.text)[1].replace(',',''))\n",
    "                    self.sought_number += foundPositions\n",
    "                try:\n",
    "                    sn = f\">> {foundPositions} positions found <<\"\n",
    "                except:\n",
    "                    sn = \">> No positions found <<\"\n",
    "                print(\n",
    "                    f\"\\r{sn.center(console.width)}\"\n",
    "                )\n",
    "            titles, countries, dates, links = [\n",
    "                soup.select(item) if repo == 'scholarshipdb' else\n",
    "                soup.find_all(class_=item)\n",
    "                for item in (c.title, c.country, c.date, c.link)\n",
    "            ]\n",
    "            assert titles != [], 'No titles found'\n",
    "            for title, country, date, link in zip(\n",
    "                    titles, countries, dates, links):\n",
    "                self.titles.append((title.text).strip())\n",
    "                self.countries.append(\n",
    "                    country.text if repo ==\n",
    "                    'scholarshipdb' else country['title'])\n",
    "                self.dates.append(date.text.replace('\\n', ''))\n",
    "                self.links.append(c.baseURL + link['href'])\n",
    "            if self.sought_number:\n",
    "                print(\n",
    "                    f\"\\rPage {page} has been fetched from {c.baseURL}!\",\n",
    "                    end=\"\")\n",
    "            return True\n",
    "        except AssertionError:\n",
    "            return False # break\n",
    "        except Exception as e:\n",
    "            # print(e)\n",
    "            return False # break\n",
    "\n",
    "    async def prepare(self):\n",
    "        for repo in self.repos:\n",
    "            print(f\"\\r{('::[ '+repo+' ]::').center(console.width, '=')}\")\n",
    "            tasks = [asyncio.create_task(self.__get_page__(repo, page)) for page in range(1, self.maxpage+1)]\n",
    "            try:\n",
    "                await asyncio.wait(tasks, return_when=asyncio.FIRST_EXCEPTION)\n",
    "            except Exception:\n",
    "                for t in tasks:\n",
    "                    t.cancel()\n",
    "\n",
    "    @property\n",
    "    def positions(self, ):\n",
    "        # asyncio.run(self.prepare())\n",
    "        try:\n",
    "            self.loop.run_until_complete(self.prepare())\n",
    "        finally:\n",
    "            self.loop.close()\n",
    "        positions = {\n",
    "            \"Country\": self.countries,\n",
    "            \"Date\": self.dates,\n",
    "            \"Title\": self.titles,\n",
    "            \"Link\": self.links\n",
    "        }\n",
    "        self.df = pd.DataFrame.from_dict(positions, orient='index').transpose()\n",
    "        self.df['timedelta'] = self.df[\"Date\"].apply(lambda x: re.sub('about|ago', '', x).strip())\n",
    "        name2days ={'minutes':'*1', 'hours':'*60', 'hour':'*60', 'days':'*1440', 'day':'*1440',\n",
    "                    'weeks':'*10080', 'week':'*10080', 'months':'*43200', 'month':'*43200'}\n",
    "        self.df.replace({'timedelta': name2days }, regex=True, inplace=True)\n",
    "        # eval is not applicable to the empty string\n",
    "        self.df['timedelta'] = self.df['timedelta'].apply(lambda x: eval(x) if x else x)\n",
    "        self.df.sort_values(by=['Country', 'timedelta', 'Title'], inplace=True)\n",
    "        self.df = self.df.drop('timedelta', axis=1).reset_index(drop=True)\n",
    "        return self.df\n",
    "\n",
    "    def save(self, output='both'):\n",
    "        \"\"\"Creates excel/csv files based on all revceived data\"\"\"\n",
    "        df = self.positions\n",
    "        if self.sought_number:\n",
    "            s  = 's' if output=='both' else ''\n",
    "            print(f\"\\r{console.width*' '}\\n>>>> {self.sought_number} positions have been found in total.\",\n",
    "            f\"Specifically, {len(df)} records of them have been saved in the following file{s}:\" , sep='\\n')\n",
    "            if output in ('csv', 'both'):\n",
    "                df.to_csv(f'{self.file_name}.csv', index=False)\n",
    "                rich.print(f'[blue]{self.file_name}.csv saved![/blue]')\n",
    "            if output in ('xlsx', 'both'):\n",
    "                df.to_excel(f'{self.file_name}.xlsx', index=False)\n",
    "                rich.print(f'[blue]{self.file_name}.xlsx saved![/blue]')\n",
    "        else:\n",
    "            rich.print('[red blink] >>> No positions found, change your keyword. <<< [/red blink]')\n",
    "\n",
    "def checkNewVersion(output:dict):\n",
    "    url = 'https://raw.github.com/Aghababaei/PhD-Seeker/master/phdseeker/constants.py'\n",
    "    response = httpx.get(url)\n",
    "    url_version = re.search('(__version__ = \"(\\d\\.\\d(\\.\\d+)?)\")', response.text, re.M)[2]\n",
    "    version = lambda v: list(map(int, v.split('.')))\n",
    "    if version(url_version) > version(__version__):\n",
    "        message = '[blink]New version ([green]{}[/green]) is available![/blink] Use `pip install --upgrade phdseeker` to update'\n",
    "        output['message'] = message.format(url_version)\n",
    "\n",
    "def main():\n",
    "    # Comma seperated list of keywords for the field of desired PhD career + presets\n",
    "    keywords = 'Computer Science, Machine Learning, Deep Learning'\n",
    "\n",
    "    ps = PhDSeeker(keywords, maxpage=10)\n",
    "    ps.save()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce585a19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
