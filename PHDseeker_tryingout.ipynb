{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d51ffd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import urllib3\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "\n",
    "urllib3.disable_warnings() # Fix the warnings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f1b2b8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 'https://scholarshipdb.net/PhD-scholarships-in-United-States?page={}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "be0b2ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page 1.\n",
      "Processing page 2.\n",
      "Processing page 3.\n",
      "Processing page 4.\n",
      "Processing page 5.\n",
      "Processing page 6.\n",
      "Processing page 7.\n",
      "Processing page 8.\n",
      "Processing page 9.\n",
      "Processing page 10.\n",
      "Processing page 11.\n",
      "Processing page 12.\n",
      "Processing page 13.\n",
      "Processing page 14.\n",
      "Processing page 15.\n",
      "Processing page 16.\n",
      "Processing page 17.\n",
      "Processing page 18.\n",
      "Processing page 19.\n",
      "Processing page 20.\n",
      "Processing page 21.\n",
      "Processing page 22.\n",
      "Processing page 23.\n",
      "Processing page 24.\n",
      "Processing page 25.\n",
      "Processing page 26.\n",
      "Processing page 27.\n",
      "Processing page 28.\n",
      "Processing page 29.\n",
      "Processing page 30.\n",
      "Processing page 31.\n",
      "Processing page 32.\n",
      "Processing page 33.\n",
      "Processing page 34.\n",
      "Processing page 35.\n",
      "Processing page 36.\n",
      "Processing page 37.\n",
      "Processing page 38.\n",
      "Processing page 39.\n",
      "Processing page 40.\n",
      "Processing page 41.\n",
      "Processing page 42.\n",
      "Processing page 43.\n",
      "Processing page 44.\n",
      "Processing page 45.\n",
      "Processing page 46.\n"
     ]
    }
   ],
   "source": [
    "allPhD = defaultdict(list)\n",
    "page = 1\n",
    "while True:\n",
    "    if page % 10 == 0:\n",
    "        print(f'Processing page {page}.')\n",
    "    url = base.format(page)\n",
    "    html_text = requests.get(url, verify=False)\n",
    "    # Create a parser instance able to parse invalid markup.\n",
    "    soup = BeautifulSoup(html_text.text, 'html.parser')\n",
    "    links = soup.select(\".list-unstyled h4 a\")\n",
    "    if len(links) == 0:\n",
    "        break\n",
    "    for link in links:\n",
    "        link = f'https://scholarshipdb.net{link.get(\"href\")}'\n",
    "        html = requests.get(link, verify=False)\n",
    "        soup2 = BeautifulSoup(html.text, 'html.parser')\n",
    "        allPhD['title'].append(soup2.find('head').title.text)\n",
    "        allPhD['location'].append(soup2.find('h2').find('a').text)\n",
    "        allPhD['keywords'].append(soup2.find('meta',attrs={\"name\": 'keywords'})['content'])\n",
    "        allPhD['meta'].append(soup2.find('meta',attrs={\"name\": 'description'})['content'])\n",
    "        allPhD['description'].append(soup2.find('div', attrs={'class': 'description'}).text.strip())\n",
    "    time.sleep(5)\n",
    "    page += 1\n",
    "allPhD = pd.DataFrame(allPhD)\n",
    "allPhD.to_pickle('scraping_scholarshipDB_dataframe.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4320efaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>keywords</th>\n",
       "      <th>meta</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3 PhD Positions, University Erlangen-Nuremberg...</td>\n",
       "      <td>University Erlangen-Nuremberg and the Charité,...</td>\n",
       "      <td>scholarship, research, uni job positions avail...</td>\n",
       "      <td>scholarship, research, uni job positions avail...</td>\n",
       "      <td>Call for Applications In the frame of an inter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PhD positions in mathematics supported by RTG ...</td>\n",
       "      <td>University of Utah, Department of Mathematics</td>\n",
       "      <td>scholarship, research, uni job positions avail...</td>\n",
       "      <td>scholarship, research, uni job positions avail...</td>\n",
       "      <td>As part of the NSF-funded Research Training Gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PhD Position in Landscape Ecology at The Unive...</td>\n",
       "      <td>The University of Texas at El Paso</td>\n",
       "      <td>scholarship, research, uni job positions avail...</td>\n",
       "      <td>scholarship, research, uni job positions avail...</td>\n",
       "      <td>Click for a hub of Extension resources related...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PhD Position in biogeochemistry/ecosystem ecol...</td>\n",
       "      <td>Utah State University</td>\n",
       "      <td>scholarship, research, uni job positions avail...</td>\n",
       "      <td>scholarship, research, uni job positions avail...</td>\n",
       "      <td>Click for a hub of Extension resources related...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Assistantship (PhD) in turtle ecology: Indiana...</td>\n",
       "      <td>Indiana State University, Department of Biology</td>\n",
       "      <td>scholarship, research, uni job positions avail...</td>\n",
       "      <td>scholarship, research, uni job positions avail...</td>\n",
       "      <td>Click for a hub of Extension resources related...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  3 PhD Positions, University Erlangen-Nuremberg...   \n",
       "1  PhD positions in mathematics supported by RTG ...   \n",
       "2  PhD Position in Landscape Ecology at The Unive...   \n",
       "3  PhD Position in biogeochemistry/ecosystem ecol...   \n",
       "4  Assistantship (PhD) in turtle ecology: Indiana...   \n",
       "\n",
       "                                            location  \\\n",
       "0  University Erlangen-Nuremberg and the Charité,...   \n",
       "1      University of Utah, Department of Mathematics   \n",
       "2                 The University of Texas at El Paso   \n",
       "3                              Utah State University   \n",
       "4    Indiana State University, Department of Biology   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  scholarship, research, uni job positions avail...   \n",
       "1  scholarship, research, uni job positions avail...   \n",
       "2  scholarship, research, uni job positions avail...   \n",
       "3  scholarship, research, uni job positions avail...   \n",
       "4  scholarship, research, uni job positions avail...   \n",
       "\n",
       "                                                meta  \\\n",
       "0  scholarship, research, uni job positions avail...   \n",
       "1  scholarship, research, uni job positions avail...   \n",
       "2  scholarship, research, uni job positions avail...   \n",
       "3  scholarship, research, uni job positions avail...   \n",
       "4  scholarship, research, uni job positions avail...   \n",
       "\n",
       "                                         description  \n",
       "0  Call for Applications In the frame of an inter...  \n",
       "1  As part of the NSF-funded Research Training Gr...  \n",
       "2  Click for a hub of Extension resources related...  \n",
       "3  Click for a hub of Extension resources related...  \n",
       "4  Click for a hub of Extension resources related...  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allPhD.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "24449830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3888888888888888"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5000/3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22e7f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import httpx\n",
    "import http3\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import rich\n",
    "from rich.console import Console\n",
    "from datetime import date\n",
    "from dataclasses import dataclass\n",
    "from bs4 import BeautifulSoup as bs\n",
    "#from pathlib import Path\n",
    "#sys.path.append(Path(__file__).parent.parent.as_posix()) # https://stackoverflow.com/questions/16981921\n",
    "from phdseeker.rich_dataframe import prettify\n",
    "from phdseeker.constants import __version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7753d8eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cannot close a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mPhDSeeker.positions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\asyncio\\base_events.py:623\u001b[0m, in \u001b[0;36mBaseEventLoop.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m--> 623\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    625\u001b[0m new_task \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m futures\u001b[38;5;241m.\u001b[39misfuture(future)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\asyncio\\base_events.py:583\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m--> 583\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis event loop is already running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: This event loop is already running",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 229>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    226\u001b[0m     ps\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 230\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    223\u001b[0m keywords \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComputer Science, Machine Learning, Deep Learning\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    225\u001b[0m ps \u001b[38;5;241m=\u001b[39m PhDSeeker(keywords, maxpage\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m--> 226\u001b[0m \u001b[43mps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mPhDSeeker.save\u001b[1;34m(self, output)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, output\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;124;03m\"\"\"Creates excel/csv files based on all revceived data\"\"\"\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositions\u001b[49m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msought_number:\n\u001b[0;32m    200\u001b[0m         s  \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mPhDSeeker.positions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloop\u001b[38;5;241m.\u001b[39mrun_until_complete(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare())\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m positions \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCountry\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcountries,\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdates,\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtitles,\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLink\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinks\n\u001b[0;32m    184\u001b[0m }\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(positions, orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\asyncio\\selector_events.py:84\u001b[0m, in \u001b[0;36mBaseSelectorEventLoop.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m---> 84\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot close a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_closed():\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot close a running event loop"
     ]
    }
   ],
   "source": [
    "# Turns off some warnings\n",
    "import urllib3\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "console = Console()\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    config = {\n",
    "        'scholarshipdb': {\n",
    "            'sought#': 'h1.title',\n",
    "            'query':\n",
    "            'https://scholarshipdb.net/scholarships/Program-PhD?page={page}&q={fields}',\n",
    "            'title': 'h4 a',\n",
    "            'country': '.list-unstyled a.text-success',\n",
    "            'date': '.list-unstyled span.text-muted',\n",
    "            'link': \".list-unstyled h4 a\",\n",
    "        },\n",
    "        'findaphd': {\n",
    "            'sought#': 'h4.course-count.d-none.d-md-block.h6.mb-0.mt-1',\n",
    "            'query':\n",
    "            'https://www.findaphd.com/phds/non-eu-students/?01w0&Keywords={fields}&PG={page}',\n",
    "            'title': \"h4 text-dark mx-0 mb-3\",\n",
    "            'country':\n",
    "            \"country-flag img-responsive phd-result__dept-inst--country-icon\",\n",
    "            'date': \"apply py-2 small\",\n",
    "            'link': \"h4 text-dark mx-0 mb-3\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    def __init__(self, repo='scholarshipdb'):\n",
    "        self.repo = repo\n",
    "\n",
    "    @classmethod\n",
    "    def repos(cls):\n",
    "        return ','.join(list(cls.config))\n",
    "\n",
    "    @property\n",
    "    def query(self):\n",
    "        return Config.config[self.repo]['query']\n",
    "\n",
    "    @property\n",
    "    def sought(self):\n",
    "        return Config.config[self.repo]['sought#']\n",
    "\n",
    "    @property\n",
    "    def title(self):\n",
    "        return Config.config[self.repo]['title']\n",
    "\n",
    "    @property\n",
    "    def link(self):\n",
    "        return Config.config[self.repo]['link']\n",
    "\n",
    "    @property\n",
    "    def country(self):\n",
    "        return Config.config[self.repo]['country']\n",
    "\n",
    "    @property\n",
    "    def date(self):\n",
    "        return Config.config[self.repo]['date']\n",
    "\n",
    "    @property\n",
    "    def baseURL(self):\n",
    "        return next(\n",
    "            re.finditer(r'^.+?[^\\/:](?=[?\\/]|$)',\n",
    "                        Config.config[self.repo]['query'])).group()\n",
    "\n",
    "\n",
    "class PhDSeeker:\n",
    "\n",
    "    def __init__(self,\n",
    "                keywords: str,\n",
    "                repos: str = 'scholarshipdb, findaphd',\n",
    "                maxpage: int = 10):\n",
    "        self.repos = map(str.strip,\n",
    "                        repos.split(','))  # 'scholarshipdb, findaphd'\n",
    "        self.keywords = keywords\n",
    "        self.fields = '%20'.join([\n",
    "            f\"\\\"{item.replace(' ', '%20')}\\\"\"\n",
    "            for item in map(str.strip, keywords.split(','))\n",
    "        ])\n",
    "        self.titles = []\n",
    "        self.countries = []\n",
    "        self.dates = []\n",
    "        self.links = []\n",
    "        self.maxpage = maxpage\n",
    "        self.file_name = f\"PhD_Positions_{date.today()}[{keywords}]\"\n",
    "        self.df = None # DataFrame of found positions\n",
    "        self.sought_number = 0\n",
    "        self.loop = asyncio.get_event_loop()\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.sought_number:\n",
    "            s = 's' if self.maxpage > 1 else ''\n",
    "            caption = \"│\" + f'Out of {self.sought_number} found Ph.D. positions, {len(self.df)} have been fetched in {self.maxpage} page{s}.'.center(console.width-2) + (\n",
    "                '│\\n└' + '─' * (console.width-2) + '┘\\n')\n",
    "            prettify(self.df[['Country', 'Date', 'Title']], clear_console=False)\n",
    "            return f'{caption}'\n",
    "\n",
    "    async def __get_page__(self, repo, page):\n",
    "        headers = {\n",
    "            'user-agent': 'curl/7.83.0',\n",
    "            'accept': '*/*',\n",
    "            'scheme': 'http',\n",
    "            # 'Content-Length': '0',\n",
    "        }\n",
    "        c = Config(repo)\n",
    "        try:\n",
    "            query = c.query.format(fields=self.fields, page=page)\n",
    "            if repo!='findaphd':\n",
    "                client = http3.AsyncClient()\n",
    "                keywords = {'verify': False}\n",
    "            else:\n",
    "                client = httpx.AsyncClient()\n",
    "                keywords = {}\n",
    "            response = await client.get(query,\n",
    "                        headers=headers,\n",
    "                        **keywords,\n",
    "                        )\n",
    "            if isinstance(client, http3.client.AsyncClient):\n",
    "                await client.close()\n",
    "            if isinstance(client, httpx._client.AsyncClient):\n",
    "                await client.aclose()\n",
    "            soup = bs(response.text, \"html.parser\")\n",
    "            if page == 1:  # get the number of sought positions\n",
    "                if (n := soup.select_one(c.sought)) is not None:\n",
    "                    foundPositions = int(re.search('(\\d+[,\\d*]*)', n.text)[1].replace(',',''))\n",
    "                    self.sought_number += foundPositions\n",
    "                try:\n",
    "                    sn = f\">> {foundPositions} positions found <<\"\n",
    "                except:\n",
    "                    sn = \">> No positions found <<\"\n",
    "                print(\n",
    "                    f\"\\r{sn.center(console.width)}\"\n",
    "                )\n",
    "            titles, countries, dates, links = [\n",
    "                soup.select(item) if repo == 'scholarshipdb' else\n",
    "                soup.find_all(class_=item)\n",
    "                for item in (c.title, c.country, c.date, c.link)\n",
    "            ]\n",
    "            assert titles != [], 'No titles found'\n",
    "            for title, country, date, link in zip(\n",
    "                    titles, countries, dates, links):\n",
    "                self.titles.append((title.text).strip())\n",
    "                self.countries.append(\n",
    "                    country.text if repo ==\n",
    "                    'scholarshipdb' else country['title'])\n",
    "                self.dates.append(date.text.replace('\\n', ''))\n",
    "                self.links.append(c.baseURL + link['href'])\n",
    "            if self.sought_number:\n",
    "                print(\n",
    "                    f\"\\rPage {page} has been fetched from {c.baseURL}!\",\n",
    "                    end=\"\")\n",
    "            return True\n",
    "        except AssertionError:\n",
    "            return False # break\n",
    "        except Exception as e:\n",
    "            # print(e)\n",
    "            return False # break\n",
    "\n",
    "    async def prepare(self):\n",
    "        for repo in self.repos:\n",
    "            print(f\"\\r{('::[ '+repo+' ]::').center(console.width, '=')}\")\n",
    "            tasks = [asyncio.create_task(self.__get_page__(repo, page)) for page in range(1, self.maxpage+1)]\n",
    "            try:\n",
    "                await asyncio.wait(tasks, return_when=asyncio.FIRST_EXCEPTION)\n",
    "            except Exception:\n",
    "                for t in tasks:\n",
    "                    t.cancel()\n",
    "\n",
    "    @property\n",
    "    def positions(self, ):\n",
    "        # asyncio.run(self.prepare())\n",
    "        try:\n",
    "            self.loop.run_until_complete(self.prepare())\n",
    "        finally:\n",
    "            self.loop.close()\n",
    "        positions = {\n",
    "            \"Country\": self.countries,\n",
    "            \"Date\": self.dates,\n",
    "            \"Title\": self.titles,\n",
    "            \"Link\": self.links\n",
    "        }\n",
    "        self.df = pd.DataFrame.from_dict(positions, orient='index').transpose()\n",
    "        self.df['timedelta'] = self.df[\"Date\"].apply(lambda x: re.sub('about|ago', '', x).strip())\n",
    "        name2days ={'minutes':'*1', 'hours':'*60', 'hour':'*60', 'days':'*1440', 'day':'*1440',\n",
    "                    'weeks':'*10080', 'week':'*10080', 'months':'*43200', 'month':'*43200'}\n",
    "        self.df.replace({'timedelta': name2days }, regex=True, inplace=True)\n",
    "        # eval is not applicable to the empty string\n",
    "        self.df['timedelta'] = self.df['timedelta'].apply(lambda x: eval(x) if x else x)\n",
    "        self.df.sort_values(by=['Country', 'timedelta', 'Title'], inplace=True)\n",
    "        self.df = self.df.drop('timedelta', axis=1).reset_index(drop=True)\n",
    "        return self.df\n",
    "\n",
    "    def save(self, output='both'):\n",
    "        \"\"\"Creates excel/csv files based on all revceived data\"\"\"\n",
    "        df = self.positions\n",
    "        if self.sought_number:\n",
    "            s  = 's' if output=='both' else ''\n",
    "            print(f\"\\r{console.width*' '}\\n>>>> {self.sought_number} positions have been found in total.\",\n",
    "            f\"Specifically, {len(df)} records of them have been saved in the following file{s}:\" , sep='\\n')\n",
    "            if output in ('csv', 'both'):\n",
    "                df.to_csv(f'{self.file_name}.csv', index=False)\n",
    "                rich.print(f'[blue]{self.file_name}.csv saved![/blue]')\n",
    "            if output in ('xlsx', 'both'):\n",
    "                df.to_excel(f'{self.file_name}.xlsx', index=False)\n",
    "                rich.print(f'[blue]{self.file_name}.xlsx saved![/blue]')\n",
    "        else:\n",
    "            rich.print('[red blink] >>> No positions found, change your keyword. <<< [/red blink]')\n",
    "\n",
    "def checkNewVersion(output:dict):\n",
    "    url = 'https://raw.github.com/Aghababaei/PhD-Seeker/master/phdseeker/constants.py'\n",
    "    response = httpx.get(url)\n",
    "    url_version = re.search('(__version__ = \"(\\d\\.\\d(\\.\\d+)?)\")', response.text, re.M)[2]\n",
    "    version = lambda v: list(map(int, v.split('.')))\n",
    "    if version(url_version) > version(__version__):\n",
    "        message = '[blink]New version ([green]{}[/green]) is available![/blink] Use `pip install --upgrade phdseeker` to update'\n",
    "        output['message'] = message.format(url_version)\n",
    "\n",
    "def main():\n",
    "    # Comma seperated list of keywords for the field of desired PhD career + presets\n",
    "    keywords = 'Computer Science, Machine Learning, Deep Learning'\n",
    "\n",
    "    ps = PhDSeeker(keywords, maxpage=10)\n",
    "    ps.save()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce585a19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
